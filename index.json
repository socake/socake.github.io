[{"content":"","date":"1 December 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"1 December 2025","externalUrl":null,"permalink":"/","section":"Startlight","summary":"","title":"Startlight","type":"page"},{"content":"","date":"1 December 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"1 December 2025","externalUrl":null,"permalink":"/categories/cka/","section":"Categories","summary":"","title":"Cka","type":"categories"},{"content":"Kubernetes知识图谱| ProcessOn免费在线作图,在线流程图,在线思维导图\n云原生工具系列\nk8s中文社区推荐文章\n一、技巧 # 复制粘贴\n# 终端面板 ctrl+shift+c/v # 除了终端外其他界面 ctrl+c/v 别名\nalias k=kubectl kubectl 自动补全(（已经不需要手动设置了，默认已有）。)\necho \u0026#34;source \u0026lt;(kubectl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc source \u0026lt;(kubectl completion bash): 这条命令会在你的当前 shell 中执行生成的 kubectl 自动补全脚本。source 命令（也可用.代替）会读取并执行指定文件中的命令\n注意写命令时不要有空格例如\u0026quot;source\u0026lt; (\u0026hellip;.)\u0026quot; 应写做“source\u0026lt;(\u0026hellip;.)”\n# 或进行手动安装 yum -y install bash-completion source /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash) echo \u0026#34;source \u0026lt;(kubectl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc yaml模版生成--dry-run=client -o yaml\nkubectl create deploy xxx --image=nginx --dry-run=client -o yaml \u0026gt; xx.yaml 强制停止，越过优雅停止\nexport now=\u0026#34;--force --grace-period 0\u0026#34; k delete pod x $now 查看帮助\nk create clusterrole --help k create rolebinding --help k scale --help k top pods --help k logs --help k drain --help 查看模kubectl explain [resource[.field]]\n打开记事本，yaml改完再vim粘贴进去\n二、题目集（一） # 1. 基于角色控制的访问控制-RBAC(4分) # 1.1 中文解释： # 创建一个名为deployment-clusterrole的clusterrole，该clusterrole只允许创建Deployment、Daemonset、Statefulset的create操作 在名字为app-team1的namespace下创建一个名为cicd-token的serviceAccount，并且将上一步创建clusterrole的权限绑定到该serviceAccount\n1.2 参考 # 使用 RBAC 鉴权 | Kubernetes\n为 Pod 配置服务账号 | Kubernetes\n1.3 解题 # # 修改默认命名空间 kubectl config get-context kubectl config set-context --current --namespace xxx # 创建clusterrole kubectl create clusterrole deploy-clusterrole --verb=create --resource=deployments,statefulsets,daemonsets # 使用yaml文件创建 [root@k8s-master01 ~]# cat dp-clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: deployment-clusterrole rules: - apiGroups: [\u0026#34;extensions\u0026#34;, \u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;,\u0026#34;statefulsets\u0026#34;,\u0026#34;daemonsets\u0026#34;] verbs: [\u0026#34;create\u0026#34;] [root@k8s-master01 ~]# kubectl create -f dp-clusterrole.yaml clusterrole.rbac.authorization.k8s.io/deployment-clusterrole created # 创建serviceAccount kubectl create sa cicd-token -n app-team1 serviceaccount/cicd-token created # 绑定权限 kubectl create rolebinding deployment-rolebinding --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token -n app-team1 # 或者使用yaml文件 apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: deployment-rolebinding namespace: app-team1 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: deployment-clusterrole subjects: - kind: ServiceAccount name: cicd-token namespace: app-team1 # 验证 kubectl auth can-i create deployment --as system:serviceaccount:app-team1:cicd-token -n app-team1 yes kubectl auth can-i create deamonset --as system:serviceaccount:app-team1:cicd-token -n app-team1 yes kubectl auth can-i create statefulset --as system:serviceaccount:app-team1:cicd-token -n app-team1 yes kubectl auth can-i create pod --as system:serviceaccount:app-team1 -n app-team1 no 1.4 实践 # clusterrol deployment-clusterrole\nserviceAccoun cicd-token\nrolebinding deployment-rolebinding\n创建clusterrole 创建serviceaccount\n绑定\u0026ndash;rolebinding\n验证 清理\nkubectl delete clusterrole deployment-clusterrole kubectl delete serviceAccount cicd-token kubectl delete rolebinding deployment-rolebinding 2. 节点维护（4分） # 2.1 中文解释： # 将ek8s-node-1节点设置为不可用，然后重新调度该节点上的所有Pod\n2.2 参考 # https://kubernetes.io/zh/docs/tasks/configure-pod-container/ https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#drain\n2.3 解题 # kubectl cordon ek8s-node-1 # 测试执行1 kubectl drain ek8s-node-1 --delete-emptydir-data --ignore-daemonset --force --dry-run=server # 腾空 kubectl drain ek8s-node-1 --delete-emptydir-data --igonre-daemonset --force 2.4 实践 # 3. k8s组件升级（7分） # 3.1 参考 # https://kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/safely-drain-node/ 3.2 解题 # # 设置维护状态 kubectl cordon k8s-master # 驱逐pod kubectl drain k8s-master --delete-emptydit-data --ignore-deamonset --force # 按题目介绍，ssh连接到一个master节点 ssh master01 sudo su - apt update apt-cache policy kubeadm |grep 1.19.0 apt install kubeadm=1.19.0-00 [--allow-change-held-packages] -y # 验证升级计划 kubuadm upgrade plan # 看到以下信息，可升级到指定版本 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.19.0 _____________________________________________________________________ # 开始升级master节点,需要留意是否需要升级etcd kuubadm upgrade apply v1.19.0 --etcd-upgrade=flase [upgrade/successful] SUCCESS! Your cluster was upgraded to \u0026#34;v1.19.0\u0026#34;. Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven\u0026#39;t already done so. # 升级kubelet和kubeproxy apt-get install -y kubelet=1.19.0-00 kubectl=1.19.0-00 [--allow-change-held-packages] systemctl deamon-reload systemctl restart kubelet # 恢复节点 kubectl uncrodon k8s-master node/k8s-master uncordoned kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 NotReady control-plane,master 11d v1.19.0 k8s-node01 Ready \u0026lt;none\u0026gt; 8d v1.18.8 k8s-node02 Ready \u0026lt;none\u0026gt; 11d v1.18.8 kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 Ready control-plane,master 11d v1.19.0 k8s-node01 Ready \u0026lt;none\u0026gt; 8d v1.18.8 k8s-node02 Ready \u0026lt;none\u0026gt; 11d v1.18.8 4. ETCD备份及恢复 # 4.1 中文解释 # 中文解释： 针对etcd实例https://127.0.0.1:2379创建一个快照，保存到 /srv/data/etcd-snapshot.db。在创建快照的过程中，如果卡住了，就键入ctrl+c终止，然后重试。 然后恢复一个已经存在的快照：/var/lib/backup/etcd-snapshot-previous.db 执行etcdctl命令的证书存放在： ca证书：/opt/KUIN00601/ca.crt 客户端证书：/opt/KUIN00601/etcd-client.crt 客户端密钥：/opt/KUIN00601/etcd-client.key\n4.2 参考 # https://kubernetes.io/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/ 4.3 解题 # kubernetes的所有数据记录在etcd中，对etcd进行备份就是对集群进行备份。 连接etcd需要证书，证书可以从apiserver获取，因为apiserver可以去连etcd。 新版本的apiserver都是以static pod方式运行，证书通过volume挂载到pod中。 具体的证书路径和备份到的路径按题目要求设置。 ssh到master节点很快，长时间没连上，可以中断重连。 恢复部分据说很容易卡住，不要花太多时间。 # 备份 export ETCDAPI_API=3 etcdctl --endpoints=\u0026#34;https://127.0.0.1:2379\u0026#34; \\ --cacert=/opt/KUIN000601/ca.crt \\ --cert=/opt/KUIN000601/etcd-client.crt \\ --key=/opt/KUIN000601/etcd-client.key snapshot save \\ /srv/data/etcd-snapshot.db # 还原 还原前最好关闭etcd,还原后重新开启 还原后etcd状态可能有问题，最好提前关掉 systemctl stop etcd mkdir /opt/backup/ -p cd /etc/kubernetes/manifests mv kube-* /opt/backup export ETCDCTL_API=3 etcdctl --endpoint=\u0026#34;https://127.0.0.1:2379\u0026#34; \\ --cacert=/opt/KUIN000601/ca.crt \\ --cert=/opt/KUIN000601/etcd-client.crt \\ --key=/opt/KUIN000601/etcd-client.key \\ snapshot restore \\ /var/lib/backup/etcd-snapshot-previous.db \\ --data-dir=/var/lib/etcd-restore # 将volume配置的path: /var/lib/etcd改成/var/lib/etcd-restore vim /etc/kubernetes/manifests/etcd.yaml volumes: - hostPath: path: /etc/kubernetes/pki/etcd type: DirectoryOrCreate name: etcd-certs - hostPath: path: /var/lib/etcd-restore # 修改目录权限 chown etcd.etcd /var/lib/etcd-restore # 还原etcd组件 mv /opt/backup/* /etc/kubenetes/manifests # 还原k8s组件 mv /opt/backup/* /etc/kubetnetes/manifests systemctl restart etcd # 其他答案 ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \\ --cacert=/opt/KUIN00601/ca.crt \\ --cert=/opt/KUIN00601/etcd-client.crt \\ --key=/opt/KUIN00601/etcd-client.key \\ snapshot save \\ /var/lib/backup/etcd-snapshot.db ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \\ --cacert=/opt/KUIN00601/ca.crt \\ --cert=/opt/KUIN00601/etcd-client.crt \\ --key=/opt/KUIN00601/etcd-client.key \\ snapshot restore \\ /var/lib/backup/etcd-snapshot-previous.db 如果是二进制安装的etcd，考试环境的etcd可能并非root用户启动的，所以可以先切换到root用户（sudo su -） 然后使用ps aux | grep etcd查看启动用户是谁和启动的配置文件是谁config-file字段指定，假设用户是etcd。所以如果是二进制安装的etcd，执行恢复时需要root权限，所以在恢复数据时，可以使用root用户恢复，之后更改恢复目录的权限：sudo chown -R etcd.etcd /var/lib/etcd-restore， 然后通过systemctl status etcd（或者ps aux | grep etcd）找到它的配置文件 （如果没有配置文件，就可以直接在etcd的service 通过systemctl status etcd即可看到文件中找到data-dir的配置），然后更改data-dir配置后，执行systemctl daemon-reload，最后使用etcd用户systemctl restart etcd即可。\n5. NetworkPolicy(7分) # 5.1 中文解释 # 创建一个名字为allow-port-from-namespace的NetworkPolicy，这个NetworkPolicy允许internal命名空间下的Pod访问该命名空间下的9000端口。 不允许不是internal命令空间的下的Pod访问 不允许访问没有监听9000端口的Pod 5.2 参考 # https://kubernetes.io/zh/docs/concepts/services-networking/network-policies/\n5.3 解题 # apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-port-from-namespace namespace: internal spec: ingress: - from: - podSelector: {} ports: - port: 9000 protocol: TCP podSelector: {} policyTypes: - Ingress 变种题目：\n在现有的namespace my-app中创建一个名为allow-port-from-namespace的NetworkPolicy 确保这个NetworkPolicy允许namespace my-app中的pods可以连接到namespace big-corp中的8080。 并且不允许不是my-app命令空间的下的Pod访问，不允许访问没有监听8080端口的Pod。 所以可以拿着上述的答案，进行稍加修改（注意namespaceSelector的labels配置。 首先需要查看big-corp命名空间有没有标签：kubectl get ns big-corp --show-labels如果有，可以更改 name: big-corp为查看到的即可。 如果没有需要添加一个label：kubectl label ns big-corp name=big-corp）： apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-port-from-namespace namespace: my-app spec: egress: - to: - namespaceSelector: matchLabels: name: big-corp ports: - protocol: TCP port: 8080 ingress: - from: - podSelector: {} ports: - port: 8080 protocol: TCP podSelector: {} policyTypes: - Ingress - Egress 变种2\nhttps://kubernetes.io/zh/docs/concepts/services-networking/network-policies/\n# 切换到指定集群 kubectl config use-context [NAME] # 查看 namespace corp-bar 的标签，如：kubernetes.io/metadata.name=corp-bar kubectl get ns --show-labels apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-port-from-namespace namespace: big-corp spec: ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: internal ports: - port: 9200 protocol: TCP podSelector: {} policyTypes: - Ingress 6. Service(7分) # 6.1 中文解释 # 重新配置一个已经存在的deployment front-end，在名字为nginx的容器里面添加一个端口配置，名字为http，暴露端口号为80/TCP，然后创建一个service，名字为front-end-svc，暴露该deployment的http端口，并且service的类型为NodePort。\n6.2 参考 # https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/ https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/ https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/****\n6.3 解题 # kubectl edit deploy front-end spec: containers: - name: nginx image: nginx # 需要加这四行 ports: - name: http containerPort: 80 protocol: TCP kubectl expose deploy front-end --name=front-end-svc --port=80 --target-port=http --type=NodePort # 或者通过文件方式创建service apiVersion: v1 kind: Service metadata: name: front-end-svc labels: app: front-end spec: type: NodePort selector: app: front-end # label需要匹配，否则访问不到。 ports: - name: http protocol: TCP port: 80 targetPort: 80 7. Ingress（7分） # 7.1 中文解释 # 在ing-internal 命名空间下创建一个ingress，名字为pong，代理的service hi，端口为5678，配置路径/hi。 验证：访问curl -kL \u0026lt;INTERNAL_IP\u0026gt;/hi会返回hi。\nIngress 是一种用于管理外部对集群内服务访问的资源。它可以根据 HTTP/HTTPS 请求的路径、主机名等规则，将流量路由到不同的后端服务\n7.2 参考 # https://kubernetes.io/zh/docs/concepts/services-networking/ingress/ https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/\n7.3 解题 # # ingressClassName需要指定为nginx apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: pong namespace: ing-internal spec: ingressClassName: nginx rules: - http: paths: - path: /hi pathType: Prefix backend: service: name: hi port: number: 5678 kubectl get ingress -n ing-internal #获取ip后使用curl验证 # 如果考试时没有出ip需要再annotation下加一行 cat ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: pong namespace: ing-internal annotations: nginx.ingress.kubernetes.io nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /hi pathType: Prefix backend: service: name: hi port: number: 5678 ingressclassname如果不指定，则会使用集群默认的指定的ingress\ningressClassName: nginx：指定 Ingress 使用的控制器类名为 nginx。这意味着 Kubernetes 将使用 Nginx Ingress 控制器来处理这个 Ingress 规则。不同的 Ingress 控制器可能有不同的功能和配置方式，这里明确使用 Nginx Ingress 控制器。 8. Deployment 扩缩容（4分） # 8.1 中文解释 # 扩容名字为loadbalancer的deployment的副本数为6\n8.2 参考 # https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/\n8.3 解题 # kubectl scale --replicas=6 deployment localbalancer kubectl edit deploy localbalancer 9. 指定节点部署-调度（4分） # 9.1 中文解释 # 创建一个Pod，名字为nginx-kusc00401，镜像地址是nginx，调度到具有disk=spinning标签的节点上\n9.2 参考 # https://kubernetes.io/zh/docs/concepts/scheduling-eviction/assign-pod-node/ https://kubernetes.io/zh/docs/tasks/configure-pod-container/assign-pods-nodes/\n9.3 解题 # vim pod-ns.yaml apiVersion: v1 kind: Pod metadata: name: nginx-kusc00401 labels: role: nginx-kusc00401 spec: nodeSelector: disk: spinning containers: - name: nginx image: nginx kubectl create -f pod-ns.yaml # 省时 kubectl run nginx-kusc00401 --image=nginx --dry-run=client -o yaml \u0026gt;9.yaml 10. 检查Node节点的健康状态（4分） # 10.1 中文解释 # 检查集群中有多少节点为Ready状态，并且去除包含NoSchedule污点的节点。之后将数字写到/opt/KUSC00402/kusc00402.txt\n10.2 参考 # https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/\n10.3 解题 # # 记录总数为 kubectl get node |grep -i ready|wc -l # 记录不可调度的节点 kubectl describe node |grep -i taints |grep -i noschedule|wc -l # 将差值写入文件 echo x \u0026gt;\u0026gt; /opt/KUSC00402/kusc00402.txt 11. 一个pod多个容器（4分） # 11.1 中文解释 # 创建一个Pod，名字为kucc1，这个Pod可能包含1-4容器，该题为四个：nginx+redis+memcached+consul\n11.2 参考 # https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/\n11.3 解题 # # 使用yaml直接创建 apiVersion: v1 kind: Pod metadata: name: kucc1 spec: containers: - image: nginx name: nginx - image: redis name: redis - image: memchached name: memcached - image: consul name: consul # 或者用dry-run=client 命令快速生成一个yaml模版。修改模板 kubectl run kuccl --image=nginx --dry-run=client -o yaml \u0026gt; 11.yaml apiVersion: v1 kind: Pod metadata: labels: run: kucc1 name: kucc1 spec: containers: - image: nginx name: nginx - image: redis name: redis - image: memcached name: memcached - image: consul name: consul dnsPolicy: ClusterFirst restartPolicy: Always 12. PersistentVolume(4分) # 12.1 中文解释 # 创建一个pv，名字为app-config，大小为2Gi，访问权限为ReadWriteMany。Volume的类型为hostPath，路径为/srv/app-config\n12.2 参考 # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/ https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/ 可以ctrl+F 搜003，会直接跳转到创建pv https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume\n12.3 解题 # apiVersion: v1 kind: PersistentVolume metadata: name: app-config labels: type: local spec: storageClassName: manual # 需要有这一项吗？题目没有要求，（可以不写） volumeMode: Filesystem capacity: storage: 2Gi accessModes: - ReadWriteMany hostPath: path: \u0026#34;/srv/app-config\u0026#34; kubectl get pv app-config 13. 监控pod度量指标（5分） # 13.1 中文解释 # 找出具有name=cpu-user的Pod，并过滤出使用CPU最高的Pod，然后把它的名字写在已经存在的 /opt/KUTR00401/KUTR00401.txt文件里（注意他没有说指定namespace。所以需要使用-A指定所以namespace）\n13.2 参考 # https://kubernetes.io/zh-cn/docs/reference/kubectl/\n13.3 解题 # kubectl top pod -A --use-protocol-buffers --selector \u0026#34;name=cpu-user\u0026#34; --sort-by NAMESPACE NAME CPU(cores) MEMORY(bytes) kube-system coredns-54d67798b7-hl8xc 7m 8Mi kube-system coredns-54d67798b7-m4m2q 6m 8Mi # 此处以pod实际名称为准，在cpu列选出最大的一个，cup数值 1,2,3 \u0026gt; 带m ，1000m=1 echo \u0026#34;coredns-54d67798b7-hl8xc\u0026#34; \u0026gt;\u0026gt; /opt/KUTR00401/KUTR00401.txt # 其他解法： kubectl get pods -A --show-labels kubectl top pods -A -l name=cpu-user --sort-by=\u0026#34;cpu\u0026#34; echo \u0026#34;[podname]\u0026#34; \u0026gt;\u0026gt; /opt/KUTR00401/KUTR00401.txt 14. 监控pod日志 # 14.1 中文解释 # 监控名为foobar的Pod的日志，并过滤出具有unable-access-website信息的行，然后将写入到 /opt/KUTR00101/foobar\n14.2 参考 # https://kubernetes.io/zh-cn/docs/reference/kubectl/\n14.3 解题 # kubectl logs foobar |grep \u0026#39;unable-access-website\u0026#39;\u0026gt;\u0026gt; /opt/KUBE0010 15. CSI \u0026amp; PersistentVolumeClaim（7分） # 15.1 中文解释 # 创建一个名字为pv-volume的pvc，指定storageClass为csi-hostpath-sc，大小为10Mi 然后创建一个Pod，名字为web-server，镜像为nginx。\n并且挂载该PVC至/usr/share/nginx/html，挂载的权限为ReadWriteOnce。之后通过 kubectl edit或者 kubectl path将pvc改成70Mi，并且记录修改记录。\n15.2 参考 # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/ https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/\n15.3 解题 # # 创建PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pv-volume spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Mi storageClassName: csi-hostpath-sc # 创建pod apiVersion: v1 kind: Pod metadata: name: web-server spec: containers: - name: nginx image: nginx volumeMounts: - mountPath: \u0026#34;/usr/share/nginx/html\u0026#34; name: pv-volume # 名字不是必须和pvc一直，也可以为my-volume volumes: - name: pv-volume # 名字不是必须和pvc一直，也可以为my-volume persistentVolumeClaim: claimName: pv-volume # 扩容 kubectl patch pvc pv-volume -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;resources\u0026#34;:{\u0026#34;requests\u0026#34;:{\u0026#34;storage\u0026#34;: \u0026#34;70Mi\u0026#34;}}}}\u0026#39; --record # 方式二 kubectl edit pvc pv-volume kubectl edit pvc pv-volume --record kubectl edit pvc pv-volume --save-config 将两处的10Mi都改为70Mi，如果是nfs会因为不支持动态扩容而失败 edit完需要等待一小会 16. sidecar(7分) # 16.1 中文解释 # 添加一个名为busybox且镜像为busybox的sidecar到一个已经存在的名为legacy-app的Pod上\n这个sidecar的启动命令为 /bin/sh, -c, 'tail -n+1 -f /var/log/legacy-app.log'。 并且这个sidecar和原有的镜像挂载一个名为logs的volume，挂载的目录为/var/log/\n16.2 参考 # https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-volume-storage/ https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/logging/\n16.3 解题 # # 导出元文件 kubectl get pod legacy-app -o yaml \u0026gt; c-sidecar.yaml apiVersion: v1 kind: Pod metadata: name: legacy-app spec: containers: - name: count image: busybox args: - /bin/sh - -c - \u0026gt; i=0; while true; do echo \u0026#34;$(date) INFO $i\u0026#34; \u0026gt;\u0026gt; /var/log/legacy-ap.log; i=$((i+1)); sleep 1; done # 基于此文件添加sidecar及volume vim c-sidecar.yaml apiVersion: v1 kind: Pod metadata: name: legacy-app spec: containers: - name: count image: busybox args: - /bin/sh - -c - \u0026gt; i=0; while true; do echo \u0026#34;$(date) INFO $i\u0026#34; \u0026gt;\u0026gt; /var/log/legacy-ap.log; i=$((i+1)); sleep 1; done # 加上下面部分 volumeMounts: - name: logs mountPath: /var/log - name: busybox image: busybox args: [/bin/sh, -c, \u0026#39;tail -n+1 -f /var/log/legacy-ap.log\u0026#39;] volumeMounts: - name: logs mountPath: /var/log volumes: - name: logs emptyDir: {} # 重新应用 kubectl delete -f c-sidecar.yaml kubectl create -f c-sidecar.yaml # 检查 kubectl logs legacy-app -c busybox 17. 集群故障排查\u0026ndash;kubelet故障（13分） # 17.1 中文解释 # 一个名为wk8s-node-0的节点状态为NotReady，让其他恢复至正常状态，并确认所有的更改开机自动完成\n17.2 解题 # # 检查服务状态 systemctl status kubelet systemctl start kubelet systemctl enable kubelet 其实这题没这么简单，一般启动kubelet后大概率是启动失败的 可能的原因： 1.kubelet二进制文件路径不对，which kubelet后和服务启动文件kubelet systemd service做个对比，看是否是这个原因 2.service文件路径和它启动的路径不一致，在启动目录下找不到service文件，可以全局搜下并做个软链接。 3.其他原因 # 再次检查wk8s-node-0是否在ready ssh master01 kubectl get nodes 17.3集群故障排查——主节点故障(13) # 这是之前的考题，现在应该没有这个题了。\n参考：\nhttps://kubernetes.io/zh/docs/tasks/configure-pod-container/static-pod/ 可能会考的题 # 题目1：nginx打标签 # labels key1=rw01 key2=rw02 思路：\nlabel pod/deployment 参考：\nhttps://kubernetes.io/zh-cn/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively 步骤：\nkubectl run hwcka-005 --image=nginx --labels key1=rw01,key2=rw02 kubectl apply -f name.yaml 题目2：deployment版本升级回退 # 1.创建deployment版本nginx 2.修改镜像1.12.0，并记录这个更新 3.回退到上个版本 思路：\n1.deployment rollout 2.--record 参考：\nhttps://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/ https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment 步骤：\nkubectl create deployment hwcka-07 --image=nginx --dry-run=client -o yaml \u0026gt; 7.yaml kubectl apply -f 7.yaml kubectl edit deployments.apps hwcka-07 --record # 修改nginx镜像为nginx:1.12.0 kubectl rollout history deployment hwcka-07 kubectl rollout undo deployment hwcka-07 --to-revision=1 # 回退前和回退后都需要edit查看下image的版本 ","date":"1 December 2025","externalUrl":null,"permalink":"/docs/linux/cka/","section":"Docs","summary":"","title":"CKA考试技巧","type":"docs"},{"content":"","date":"1 December 2025","externalUrl":null,"permalink":"/docs/","section":"Docs","summary":"","title":"Docs","type":"docs"},{"content":"这是我的第二篇 Hugo 文章！\n标题 # Hugo 使用 CommonMark Markdown 规范，支持各种 Markdown 语法。\n列表项 1 列表项 2 列表项 3 粗体文本 和 斜体文本。\n","date":"30 November 2025","externalUrl":null,"permalink":"/posts/my-second-post/","section":"Posts","summary":"","title":"My Second Post","type":"posts"},{"content":" 这是我的第一篇 Hugo 文章！ # 标题 # s Hugo 使用 CommonMark Markdown 规范，支持各种 Markdown 语法。\n列表项 1 列表项 2 列表项 3 粗体文本 和 斜体文本。\n","date":"30 November 2025","externalUrl":null,"permalink":"/posts/my-first-post/","section":"Posts","summary":"","title":"My First Post","type":"posts"},{"content":"This is the introduction.\nFirst H2 Title # This is content under the first H2 title.\nSecond H2 Title # This is content under the second H2 title.\n","date":"1 December 2023","externalUrl":null,"permalink":"/posts/test-toc/","section":"Posts","summary":"","title":"TOC Test Post","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]